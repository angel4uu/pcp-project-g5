# ğŸŸ¡ HU-04: OptimizaciÃ³n de Inferencia - GuÃ­a RÃ¡pida para Equipo 2

> **Este documento es para el Equipo 2 (2 personas) que trabaja en optimizaciÃ³n con TensorRT/ONNX**

## ğŸ¯ Objetivo de la Historia de Usuario

Convertir el modelo YOLO de PyTorch a formatos optimizados (ONNX, TensorRT) para lograr **2-5x speedup** sin pÃ©rdida de precisiÃ³n (mAP).

---

## ğŸš€ Arranque en 5 Minutos (Windows)

### Paso 1: Abrir PowerShell y ejecutar

```powershell
# Navegar a la carpeta del proyecto
cd "C:\Users\USUARIO\Documents\Proyectos\Construccion-software\pcp-project-g5"

# Ejecutar script de setup (instala todo automÃ¡ticamente)
.\setup_hu04.ps1
```

**Output esperado**:
```
================================
ğŸš€ SETUP HU-04: WINDOWS POWERSHELL
================================

[0/4] Verificando Python...
  âœ… Python 3.11.x

[1/4] Preparando entorno virtual...
  âœ… Entorno virtual creado

[2/4] Actualizando pip...
  âœ… pip actualizado

[3/4] Instalando dependencias HU-04...
  âœ… (mÃºltiples paquetes instalÃ¡ndose...)

[4/4] Verificando instalaciÃ³n...
  âœ… SETUP CORRECTO - Â¡Listo para HU-04!

ğŸ“‹ PrÃ³ximos pasos:
1ï¸âƒ£  Exportar modelo...
...
```

---

## ğŸ“Š Flujo de Trabajo TÃ­pico (2 personas, 1 semana)

```
DÃA 1-2: ExportaciÃ³n
â”œâ”€ Tarea 1: Exportar YOLO â†’ ONNX
â”‚  â””â”€ Responsable: Persona A
â”‚  â””â”€ Comando: python scripts/export_to_tensorrt.py --model model.pt --output models --benchmark
â”‚  â””â”€ Validar: archivo models/model.onnx existe
â”‚
â”œâ”€ Tarea 2: Benchmarking Python
â”‚  â””â”€ Responsable: Persona B
â”‚  â””â”€ Comando: python models/benchmark_onnx_vs_pytorch.py
â”‚  â””â”€ Resultado: speedup 1.5-3x esperado
â”‚
â””â”€ ReuniÃ³n: Revisar speedup vs PyTorch

DÃA 3-4: TensorRT (opcional, si GPU disponible)
â”œâ”€ Tarea 3: Instalar TensorRT
â”‚  â””â”€ Responsable: Persona A
â”‚  â””â”€ URL: https://developer.nvidia.com/tensorrt
â”‚  â””â”€ Validar: trtexec --help
â”‚
â”œâ”€ Tarea 4: Exportar ONNX â†’ TensorRT
â”‚  â””â”€ Responsable: Persona B
â”‚  â””â”€ Comando: python models/convert_to_tensorrt.py
â”‚  â””â”€ Resultado: models/model.fp16.engine
â”‚
â””â”€ ReuniÃ³n: Validar engine TensorRT

DÃA 5-6: ValidaciÃ³n
â”œâ”€ Tarea 5: Validar mAP y precisiÃ³n
â”‚  â””â”€ Responsable: Ambos
â”‚  â””â”€ Comando: python scripts/validate_models.py
â”‚  â””â”€ Criterios: mAP loss < 1%, similitud > 98%
â”‚
â””â”€ Tarea 6: Documentar resultados
   â””â”€ Crear reporte final con benchmarks

DÃA 7: C++ + CUDA (avanzado, si tiempo disponible)
â””â”€ Compilar pipeline C++ con TensorRT
   â””â”€ Comando: cd scripts && mkdir build && cd build && cmake .. && make
   â””â”€ Ejecutar: ./yolo_tensorrt_detector model.fp16.engine videos/prueba2.mp4
```

---

## ğŸ’» Comandos Principales

### Exportar YOLO a ONNX

```powershell
# Activar entorno (si no estÃ¡ activo)
.\.venv\Scripts\Activate.ps1

# Exportar modelo
python scripts/export_to_tensorrt.py `
    --model model.pt `
    --output models `
    --benchmark
```

**Archivos generados**:
- `models/model.onnx` (modelo compilado)
- `models/benchmark_onnx_vs_pytorch.py` (script para benchmark)

---

### Benchmarking (medir velocidad)

```powershell
# Ejecutar benchmark de PyTorch vs ONNX
python models/benchmark_onnx_vs_pytorch.py
```

**Output esperado**:
```
============================================================
BENCHMARK: PyTorch vs ONNX Runtime
============================================================

â±ï¸  Benchmark PyTorch (GPU)
  Latencia: 45.32 Â± 2.15 ms
  FPS: 22.07

â±ï¸  Benchmark ONNX Runtime (CUDA)
  Latencia: 18.45 Â± 1.50 ms
  FPS: 54.20

ğŸ“Š SPEEDUP ONNX: 2.46x
   PyTorch: 45.32 ms (22.07 FPS)
   ONNX:    18.45 ms (54.20 FPS)
```

---

### Validar PrecisiÃ³n

```powershell
# Comparar outputs PyTorch vs ONNX
python scripts/validate_models.py `
    --pytorch model.pt `
    --onnx models/model.onnx `
    --images scripts/images
```

**Criterios de aceptaciÃ³n**:
- âœ… Similitud â‰¥ 98% (outputs casi idÃ©nticos)
- âœ… mAP loss < 1% (precisiÃ³n no decrece significativamente)

---

## ğŸ“‚ Estructura de Archivos Generados

```
pcp-project-g5/
â”œâ”€â”€ models/                          # Directorio de exportaciÃ³n
â”‚   â”œâ”€â”€ model.onnx                  # âœ… Modelo ONNX compilado
â”‚   â”œâ”€â”€ model.fp16.engine           # (generado por TensorRT)
â”‚   â”œâ”€â”€ model.fp32.engine           # (alternativa precisiÃ³n)
â”‚   â”œâ”€â”€ benchmark_onnx_vs_pytorch.py    # (auto-generado)
â”‚   â””â”€â”€ convert_to_tensorrt.py      # (auto-generado)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ export_to_tensorrt.py       # ğŸ”„ Script principal exportaciÃ³n
â”‚   â”œâ”€â”€ validate_models.py          # ğŸ”„ Validador
â”‚   â”œâ”€â”€ check_hu04_setup.py         # ğŸ§ª Smoke test
â”‚   â”œâ”€â”€ yolo_tensorrt_detector.cpp  # C++ pipeline
â”‚   â””â”€â”€ CMakeLists.txt              # Build system
â”œâ”€â”€ HU-04-OPTIMIZACION.md           # ğŸ“– DocumentaciÃ³n completa
â””â”€â”€ requirements-hu04.txt           # ğŸ“‹ Dependencias
```

---

## ğŸ” Troubleshooting

### Error: "No module named 'cv2'"
```powershell
pip install opencv-contrib-python
```

### Error: "No module named 'ultralytics'"
```powershell
pip install ultralytics
```

### Error: "CUDA not available"
```powershell
# Es normal, funcionarÃ¡ con CPU (mÃ¡s lento)
# Para habilitar CUDA:
pip uninstall torch -y
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### Error: "trtexec not found"
```powershell
# Descargar TensorRT: https://developer.nvidia.com/tensorrt
# AÃ±adir a PATH:
$env:PATH += ";C:\Program Files\TensorRT\bin"
```

---

## ğŸ“Š MÃ©tricas a Recolectar

Durante la ejecuciÃ³n, documentar:

| MÃ©trica | PyTorch | ONNX | TensorRT |
|---------|---------|------|----------|
| FPS | ? | ? | ? |
| Latencia (ms) | ? | ? | ? |
| mAP@0.5 | ? | ? | ? |
| Memory (MB) | ? | ? | ? |
| TamaÃ±o modelo | ? | ? | ? |

**Template de reporte**:
```
FECHA: 2025-11-06
RESPONSABLE: Persona A + Persona B

RESULTADOS:
- FPS ONNX vs PyTorch: 2.46x speedup âœ“
- Similitud outputs: 98.5% âœ“
- mAP loss: 0.3% âœ“
- Status: LISTO PARA PRODUCCIÃ“N âœ“
```

---

## ğŸ“ Conceptos Clave

### ONNX (Open Neural Network Exchange)
- Formato neutral para redes neuronales
- Interoperable (PyTorch â†’ ONNX â†’ TensorRT, ONNX.js, etc.)
- TÃ­picamente 1-2x mÃ¡s rÃ¡pido que PyTorch puro

### TensorRT
- Optimizador de Nvidia para CUDA/GPU
- CompilaciÃ³n JIT de modelos
- TÃ­picamente 2-5x mÃ¡s rÃ¡pido que ONNX
- Requiere GPU Nvidia

### mAP (mean Average Precision)
- MÃ©trica de precisiÃ³n para detecciÃ³n de objetos
- Rango: 0-100%
- Aceptable pÃ©rdida: < 1%

---

## ğŸ“ Contacto y Ayuda

**Preguntas sobre exportaciÃ³n**:
- Ver: `scripts/export_to_tensorrt.py` (comentarios en cÃ³digo)
- Ejecutar: `python scripts/export_to_tensorrt.py --help`

**Preguntas sobre validaciÃ³n**:
- Ver: `scripts/validate_models.py`
- Ejecutar: `python scripts/validate_models.py --help`

**Preguntas sobre compilaciÃ³n C++**:
- Ver: `scripts/CMakeLists.txt`
- Ver: `HU-04-OPTIMIZACION.md` (secciÃ³n C++)

**DocumentaciÃ³n completa**:
```powershell
cat HU-04-OPTIMIZACION.md
```

---

## âœ… Checklist Final

- [ ] Setup completado (`.\setup_hu04.ps1` sin errores)
- [ ] Smoke test pasado (`python scripts/check_hu04_setup.py`)
- [ ] Modelo ONNX exportado (`models/model.onnx` existe)
- [ ] Benchmark ejecutado (speedup documentado)
- [ ] ValidaciÃ³n mAP completada (criterios cumplidos)
- [ ] Reporte final generado
- [ ] DocumentaciÃ³n actualizada

---

**Ãšltima actualizaciÃ³n**: 2025-11-06  
**Responsables**: Equipo 2 (OptimizaciÃ³n)  
**Estado**: ğŸŸ¡ En Progreso
