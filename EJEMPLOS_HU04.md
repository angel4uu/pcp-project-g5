#!/usr/bin/env python3
"""
Ejemplo completo de cÃ³mo usar HU-04: OptimizaciÃ³n de Inferencia

Este script demuestra:
1. Cargar modelo PyTorch (baseline)
2. Exportar a ONNX
3. Comparar velocidad (benchmarking)
4. Validar precisiÃ³n
"""

import os
import sys
import time
import numpy as np
from pathlib import Path

def example_1_load_pytorch_model():
    """Ejemplo 1: Cargar modelo PyTorch."""
    print("\n" + "=" * 70)
    print("EJEMPLO 1: Cargar modelo PyTorch")
    print("=" * 70)
    
    print("\nðŸ“– CÃ³digo:")
    print("""
from ultralytics import YOLO
import torch

# Verificar CUDA
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Usando: {device}")

# Cargar modelo
model = YOLO('model.pt')
model.to(device)

# Hacer inferencia
results = model('image.jpg', verbose=False)
print(f"Detecciones: {len(results[0].boxes)}")
    """)
    
    print("\nâ±ï¸  Tiempo esperado: 45 ms/frame (CPU), 15 ms/frame (GPU)")
    print("ðŸ“Š Output: resultados con bounding boxes y confianzas")

def example_2_export_to_onnx():
    """Ejemplo 2: Exportar a ONNX."""
    print("\n" + "=" * 70)
    print("EJEMPLO 2: Exportar modelo YOLO a ONNX")
    print("=" * 70)
    
    print("\nðŸ“– CÃ³digo:")
    print("""
from ultralytics import YOLO
import torch

# Cargar modelo
model = YOLO('model.pt')

# Exportar a ONNX
exported_path = model.export(
    format='onnx',
    opset=12,
    simplify=True,
    device=0 if torch.cuda.is_available() else 'cpu'
)
print(f"âœ… Exportado a: {exported_path}")
    """)
    
    print("\nâ±ï¸  Tiempo esperado: 2-5 minutos (solo una vez)")
    print("ðŸ“Š Output: archivo models/model.onnx (~48 MB)")
    print("ðŸ’¡ Ventaja: formato interoperable, 1.5x rÃ¡pido vs PyTorch")

def example_3_benchmark():
    """Ejemplo 3: Benchmarking PyTorch vs ONNX."""
    print("\n" + "=" * 70)
    print("EJEMPLO 3: Benchmarking - PyTorch vs ONNX")
    print("=" * 70)
    
    print("\nðŸ“– CÃ³digo:")
    print("""
import numpy as np
import time
from ultralytics import YOLO
import onnxruntime as ort

# Setup
model = YOLO('model.pt')
onnx_session = ort.InferenceSession(
    'models/model.onnx',
    providers=['CUDAExecutionProvider', 'CPUExecutionProvider']
)
dummy_input = np.random.randn(1, 3, 640, 640).astype(np.float32)

# Benchmark PyTorch
pytorch_times = []
for _ in range(100):
    start = time.perf_counter()
    _ = model(dummy_input)
    pytorch_times.append((time.perf_counter() - start) * 1000)

# Benchmark ONNX
onnx_times = []
for _ in range(100):
    start = time.perf_counter()
    input_name = onnx_session.get_inputs()[0].name
    _ = onnx_session.run(None, {input_name: dummy_input})
    onnx_times.append((time.perf_counter() - start) * 1000)

# Resultados
pytorch_avg = np.mean(pytorch_times)
onnx_avg = np.mean(onnx_times)
speedup = pytorch_avg / onnx_avg

print(f"PyTorch: {pytorch_avg:.2f} ms")
print(f"ONNX:    {onnx_avg:.2f} ms")
print(f"Speedup: {speedup:.2f}x")
    """)
    
    print("\nðŸ“Š Output esperado:")
    print("   PyTorch: 45.32 ms")
    print("   ONNX:    18.45 ms")
    print("   Speedup: 2.46x âœ…")

def example_4_validate_precision():
    """Ejemplo 4: Validar precisiÃ³n (similitud outputs)."""
    print("\n" + "=" * 70)
    print("EJEMPLO 4: Validar PrecisiÃ³n (mAP)")
    print("=" * 70)
    
    print("\nðŸ“– Concepto:")
    print("""
La precisiÃ³n se mide comparando:
- PyTorch (baseline): outputs "verdaderos"
- ONNX (exportado): outputs a validar

MÃ©trica: mAP (mean Average Precision)
- rango: 0-100%
- aceptable: >98% similitud vs PyTorch
    """)
    
    print("\nðŸ“– CÃ³digo:")
    print("""
from ultralytics import YOLO
import onnxruntime as ort
import cv2
import numpy as np

model = YOLO('model.pt')
onnx_session = ort.InferenceSession('models/model.onnx')

# Comparar en 10 imÃ¡genes de prueba
similarities = []
for image_path in image_paths[:10]:
    # PyTorch
    pytorch_results = model(image_path, verbose=False)
    pytorch_dets = len(pytorch_results[0].boxes)
    
    # ONNX
    image = cv2.imread(image_path)
    image = cv2.resize(image, (640, 640))
    image = image.astype(np.float32) / 255.0
    image = np.transpose(image, (2, 0, 1))[np.newaxis, :]
    
    input_name = onnx_session.get_inputs()[0].name
    onnx_outputs = onnx_session.run(None, {input_name: image})
    onnx_dets = len(onnx_outputs[0]) if onnx_outputs else 0
    
    similarity = 1.0 - abs(pytorch_dets - onnx_dets) / max(pytorch_dets, onnx_dets, 1)
    similarities.append(similarity)

avg_similarity = np.mean(similarities) * 100
print(f"Similitud promedio: {avg_similarity:.2f}%")
print(f"Status: {'âœ… ACEPTADO' if avg_similarity > 98 else 'âŒ REVISAR'}")
    """)
    
    print("\nðŸ“Š Output esperado:")
    print("   Similitud: 98.50%")
    print("   Status: âœ… ACEPTADO")

def example_5_tensorrt_export():
    """Ejemplo 5: Exportar ONNX a TensorRT (avanzado)."""
    print("\n" + "=" * 70)
    print("EJEMPLO 5: Exportar ONNX a TensorRT (Avanzado)")
    print("=" * 70)
    
    print("\nðŸ“– Requisitos:")
    print("""
- TensorRT 8.6+ instalado
- CUDA 11.8+ disponible
- trtexec en PATH
    """)
    
    print("\nðŸ“– Comando:")
    print("""
trtexec --onnx=models/model.onnx \\
        --saveEngine=models/model.fp16.engine \\
        --workspace=1024 \\
        --fp16
    """)
    
    print("\nâ±ï¸  Tiempo esperado: 5-15 minutos")
    print("ðŸ“Š Output: archivo models/model.fp16.engine (~15 MB)")
    print("ðŸ’¡ Ventaja: 3-5x rÃ¡pido vs PyTorch, solo GPU Nvidia")

def example_6_cpp_tensorrt():
    """Ejemplo 6: Usar TensorRT desde C++ (avanzado)."""
    print("\n" + "=" * 70)
    print("EJEMPLO 6: Pipeline C++ + TensorRT + CUDA (Avanzado)")
    print("=" * 70)
    
    print("\nðŸ“– CompilaciÃ³n:")
    print("""
cd scripts
mkdir build && cd build
cmake .. -DTENSORRT_ROOT=/path/to/tensorrt
cmake --build . --config Release -j8
    """)
    
    print("\nðŸ“– EjecuciÃ³n:")
    print("""
./yolo_tensorrt_detector \\
    ../models/model.fp16.engine \\
    ../scripts/videos/prueba2.mp4 \\
    0.5
    """)
    
    print("\nðŸ“Š Output esperado:")
    print("""
ðŸ“‚ Cargando engine TensorRT: ../models/model.fp16.engine
âœ… Engine cargado
   Input: 2560000 elementos
   Output: 25200 elementos

â±ï¸  Tiempo inferencia: 15 ms

ðŸ“Š RESULTADOS
======================================================================
Frames procesados: 300
Rostros detectados: 542
FPS promedio: 66.67
Latencia promedio: 15.00 ms/frame
    """)

def example_7_complete_pipeline():
    """Ejemplo 7: Pipeline completo (inicio a fin)."""
    print("\n" + "=" * 70)
    print("EJEMPLO 7: Pipeline Completo (Inicio a Fin)")
    print("=" * 70)
    
    print("\nðŸ“‹ Checklist de ejecuciÃ³n:")
    print("""
1. Setup (5 min)
   â”œâ”€ .\\setup_hu04.ps1
   â””â”€ python scripts/check_hu04_setup.py

2. ExportaciÃ³n ONNX (2 horas)
   â””â”€ python scripts/export_to_tensorrt.py --model model.pt --output models --benchmark

3. Benchmarking (30 min)
   â””â”€ python models/benchmark_onnx_vs_pytorch.py

4. ValidaciÃ³n mAP (1 hora)
   â””â”€ python scripts/validate_models.py

5. TensorRT (opcional, 3 horas)
   â”œâ”€ Descargar TensorRT desde https://developer.nvidia.com/tensorrt
   â”œâ”€ Instalar y configurar PATH
   â””â”€ python models/convert_to_tensorrt.py

6. C++ + CUDA (opcional, 2-4 horas)
   â”œâ”€ cd scripts && mkdir build && cd build
   â”œâ”€ cmake .. -DTENSORRT_ROOT=/path/to/tensorrt
   â”œâ”€ cmake --build . --config Release
   â””â”€ ./yolo_tensorrt_detector models/model.fp16.engine videos/prueba2.mp4

7. Reporte (1 hora)
   â””â”€ Documentar todos los benchmarks y resultados
    """)

def main():
    print("\n" + "=" * 70)
    print("ðŸŽ“ EJEMPLOS PRÃCTICOS - HU-04: OPTIMIZACIÃ“N DE INFERENCIA")
    print("=" * 70)
    
    # Listar ejemplos
    examples = [
        ("1", "Cargar modelo PyTorch", example_1_load_pytorch_model),
        ("2", "Exportar a ONNX", example_2_export_to_onnx),
        ("3", "Benchmarking", example_3_benchmark),
        ("4", "ValidaciÃ³n PrecisiÃ³n", example_4_validate_precision),
        ("5", "Exportar a TensorRT", example_5_tensorrt_export),
        ("6", "Pipeline C++ TensorRT", example_6_cpp_tensorrt),
        ("7", "Pipeline Completo", example_7_complete_pipeline),
    ]
    
    print("\nðŸ“š Ejemplos disponibles:\n")
    for num, title, _ in examples:
        print(f"  {num}. {title}")
    
    print("\n" + "=" * 70)
    
    # Mostrar todos los ejemplos
    for num, title, func in examples:
        func()
    
    # Resumen final
    print("\n" + "=" * 70)
    print("ðŸ“– RESUMEN")
    print("=" * 70)
    
    print("""
PyTorch  â”€â”€(export)â”€â”€>  ONNX  â”€â”€(convert)â”€â”€>  TensorRT
â”œâ”€ 45 ms/frame           â”œâ”€ 18 ms/frame        â””â”€ 15 ms/frame
â”œâ”€ Baseline              â”œâ”€ 2.5x speedup       â”œâ”€ 3x speedup
â””â”€ mAP: 100%             â”œâ”€ 1.5% loss          â””â”€ <2% loss
                         â””â”€ GPU/CPU            â””â”€ Solo GPU Nvidia

RecomendaciÃ³n:
â”œâ”€ Desarrollo: PyTorch
â”œâ”€ ProducciÃ³n (CPU): ONNX (1.5-2x rÃ¡pido)
â””â”€ ProducciÃ³n (GPU): TensorRT (2-5x rÃ¡pido)
    """)
    
    print("=" * 70)
    print("\nâœ… Ejecuta los scripts en este orden:")
    print("   1. python scripts/export_to_tensorrt.py")
    print("   2. python models/benchmark_onnx_vs_pytorch.py")
    print("   3. python scripts/validate_models.py")
    print("\nðŸ’¡ Ver documentaciÃ³n completa: HU-04-OPTIMIZACION.md")
    print("=" * 70 + "\n")

if __name__ == "__main__":
    main()
